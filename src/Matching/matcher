
# Purpose: Contains functions to compare and analyze resumes against job descriptions.

# --- Imports ---

# For numerical similarity using sentence embeddings
# Install using: pip install sentence-transformers
from sentence_transformers import SentenceTransformer, util

# To access the configured Gemini model (assuming it's set up in client.py)
# Using 'try...except' allows the script to potentially run even if llm parts fail or are unused.
try:
    # Use relative import assuming client.py is in ../llm/ relative to this file
    from ..llm.client import get_model
    # Get the globally configured LLM model instance when this module loads
    # Note: This assumes client.py has already run and configured the model.
    # If client.py might not have run yet, call get_model() inside the LLM function instead.
    llm_model = get_model()
    # Check if the model was loaded successfully
    if not llm_model:
        print("Warning: LLM model not loaded successfully from client.py during matcher.py import. LLM matching will fail unless client.py runs later.")
except ImportError:
    print("Warning: Could not import get_model from ..llm.client. LLM matching features will not be available.")
    # Ensure llm_model is None if import fails, so checks later don't cause errors
    llm_model = None
except Exception as e:
    print(f"An unexpected error occurred during LLM model import/retrieval in matcher.py: {e}")
    llm_model = None


# --- Sentence Transformer Model Loading ---
# Load the Sentence Transformer model ONCE when the module is loaded.
# This is much more efficient than loading it inside the function every time it's called.
# 'all-MiniLM-L6-v2' is a good starting point - fast and reasonably accurate for semantic similarity.
embedding_model = None # Initialize to None
try:
    print("Loading Sentence Transformer model (all-MiniLM-L6-v2)...")
    # Wrap model loading in try-except in case download or loading fails
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    print("✅ Sentence Transformer model loaded successfully.")
except Exception as e:
    print(f"Error loading Sentence Transformer model: {e}. Embedding similarity will not be available.")
    # embedding_model remains None if loading fails


# --- Function 1: Numerical Embedding Similarity ---
def compute_embedding_similarity(resume_text, jd_text):
    """
    Computes the semantic similarity between resume and job description text
    using sentence embeddings (SentenceTransformer model).

    Args:
        resume_text (str): The text content of the resume.
        jd_text (str): The text content of the job description.

    Returns:
        float | None: The cosine similarity score (typically between 0.0 and 1.0 for text),
                      or None if the embedding model failed to load or an error occurs during computation.
    """
    # Check if the embedding model was loaded successfully during module import
    if embedding_model is None:
        print("Error: Sentence Transformer model not available for similarity computation.")
        return None # Return None if model isn't loaded

    # Ensure inputs are strings
    if not isinstance(resume_text, str) or not isinstance(jd_text, str):
        print("Error: Both resume_text and jd_text must be strings.")
        return None

    try:
        print("Encoding resume and JD text for similarity calculation...")
        # Encode both texts into vector embeddings. convert_to_tensor=True is efficient for util.cos_sim.
        embeddings = embedding_model.encode([resume_text, jd_text], convert_to_tensor=True)

        # Calculate the cosine similarity between the two embeddings
        # embeddings[0] is the resume embedding, embeddings[1] is the JD embedding
        # .item() extracts the scalar value from the resulting tensor (e.g., tensor([[0.75]]))
        similarity_score = util.cos_sim(embeddings[0], embeddings[1]).item()
        print(f"Computed embedding similarity score: {similarity_score:.4f}")

        # Ensure score is within expected range (cosine sim is -1 to 1, but for text often 0-1)
        # Clamping might be useful if you want to strictly enforce a 0-1 range for display
        # similarity_score = max(0.0, min(1.0, similarity_score))

        return similarity_score # Return the calculated float score

    except Exception as e:
        print(f"Error computing embedding similarity: {e}")
        return None # Return None if an error occurs during encoding/similarity calculation


# --- Function 2: LLM-based Matching Analysis ---
def match_resume_with_jd_llm(resume_text, jd_text):
    """
    Uses a Generative Language Model (Gemini) to analyze the match between
    a resume and a job description, providing a score, explanation, and missing factors.

    Args:
        resume_text (str): The text content of the resume.
        jd_text (str): The text content of the job description.

    Returns:
        str | None: The analysis text generated by the LLM, or None if the LLM
                    model is not available or an error occurs during generation.
    """
    # Check if the LLM model was successfully retrieved or instantiated earlier.
    # Re-check here in case client.py runs after this module is imported.
    current_llm_model = get_model() # Call get_model again to get the latest state

    if not current_llm_model:
        print("Error: LLM model not available. Cannot perform LLM-based matching.")
        return None

    # Ensure inputs are strings
    if not isinstance(resume_text, str) or not isinstance(jd_text, str):
        print("Error: Both resume_text and jd_text must be strings for LLM analysis.")
        return None

    # Define the prompt for the LLM
    # This prompt guides the LLM to act as a recruiter and provide specific outputs.
    # Using clear formatting and instructions improves the reliability of the output.
    prompt = f"""
    Analyze the following resume and job description. Act as an expert talent acquisition specialist providing a concise evaluation for a hiring manager.

    **Resume Text:**
    ---
    {resume_text}
    ---

    **Job Description Text:**
    ---
    {jd_text}
    ---

    **Your Task:**
    1.  Provide an overall **Match Score** (out of 100) representing the candidate's suitability based *only* on the provided texts. Base the score on how well the resume demonstrates the qualifications and experience listed in the job description.
    2.  Write a brief **Explanation** (2-4 sentences) justifying the score, highlighting key alignments or significant gaps.
    3.  List the most critical **Missing Factors** (specific keywords, skills, qualifications, or years of experience mentioned in the JD but seemingly absent or insufficient in the candidate's resume). If there are no significant missing factors, state "None apparent." Use bullet points for the list.
    4.  Refer to the person who submitted the resume only as "the candidate" or "the applicant". Do not invent or use a name found in the resume text.

    **Output Format:**
    Strictly follow this format, including the labels:

    Match Score: [Score]/100
    Explanation: [Your explanation here]
    Missing Factors:
    * [Missing factor 1]
    * [Missing factor 2]
    * [Or "None apparent."]
    """

    try:
        print("Sending request to LLM for resume-JD analysis...")
        # Send the prompt to the Gemini model instance retrieved earlier
        response = current_llm_model.generate_content(prompt)

        # Basic check if response has text (some APIs might return empty responses on errors/filters)
        if hasattr(response, 'text') and response.text:
             analysis_text = response.text
             print("✅ LLM analysis received.")
             return analysis_text
        else:
             # Handle cases where the response might be blocked or empty
             print("⚠️ LLM response received but contains no text. It might have been blocked or empty.")
             # Check for safety ratings or prompt feedback if available
             if hasattr(response, 'prompt_feedback'):
                 print(f"Prompt Feedback: {response.prompt_feedback}")
             return "Error: LLM response was empty or blocked. Please check content safety settings or modify input."


    except Exception as e:
        # Handle potential errors during the API call (e.g., network issues, API errors, quota limits)
        print(f"Error generating LLM response for matching: {e}")
        return f"Error during LLM analysis: {e}" # Return error message for debugging




# --- Example Usage ---
# This block runs only when the script is executed directly (e.g., python src/analysis/matcher.py)
# It's useful for testing the functions in isolation.
if __name__ == '__main__':
    # Dummy text for testing
    sample_resume = """
    Alice Wonderland
    alice.wonderland@example.com | 555-1234 | linkedin.com/in/alicew

    Summary
    Highly motivated Software Engineer with 4 years of experience in developing scalable web applications using Python, Django, and JavaScript. Proven ability to work in fast-paced environments and deliver high-quality code. Seeking a challenging role in a growth-oriented company.

    Experience
    Software Engineer | Tech Solutions Inc. | 2020 - Present
    * Developed and maintained backend services for e-commerce platform using Python and Django REST Framework.
    * Collaborated with frontend team using React to integrate APIs.
    * Implemented unit and integration tests, improving code coverage by 20%.
    * Utilized PostgreSQL for database management and Git for version control.

    Junior Developer | Web Creators LLC | 2019 - 2020
    * Assisted senior developers in building website components using HTML, CSS, and JavaScript.
    * Participated in code reviews and bug fixing.

    Education
    Bachelor of Science in Computer Science | State University | 2019

    Skills
    Programming Languages: Python, JavaScript, HTML, CSS, SQL
    Frameworks/Libraries: Django, Django REST Framework, React, Node.js (basic)
    Databases: PostgreSQL, SQLite
    Tools: Git, Docker (basic), Jira
    """

    sample_jd = """
    Job Title: Senior Python Backend Engineer
    Location: Remote

    About Us:
    We are a leading FinTech company revolutionizing online payments. We are looking for a talented Senior Python Backend Engineer to join our core platform team.

    Responsibilities:
    * Design, develop, and maintain robust and scalable backend systems using Python and Flask or Django.
    * Build and integrate RESTful APIs for internal and external services.
    * Work closely with product managers and frontend engineers.
    * Optimize application performance and database queries (PostgreSQL required).
    * Implement security best practices.
    * Mentor junior engineers and participate in code reviews.
    * Utilize cloud platforms like AWS for deployment and monitoring.

    Qualifications:
    * 5+ years of professional software development experience.
    * Strong proficiency in Python and backend frameworks (Django or Flask).
    * Solid experience with relational databases, particularly PostgreSQL.
    * Experience designing and consuming RESTful APIs.
    * Familiarity with cloud platforms (AWS preferred).
    * Experience with containerization (Docker) is a strong plus.
    * Excellent problem-solving skills and communication abilities.
    * Bachelor's or Master's degree in Computer Science or related field.
    """

    print("\n--- Testing Embedding Similarity ---")
    similarity = compute_embedding_similarity(sample_resume, sample_jd)
    if similarity is not None:
        # Format the score as a percentage for potentially clearer interpretation
        print(f"Embedding Similarity Score: {similarity:.4f} (or {similarity*100:.2f}%)")
    else:
        print("Embedding similarity calculation failed.")

    print("\n--- Testing LLM Analysis ---")
    # Ensure your .env file is set up with GEMAI_API_KEY and client.py ran successfully for this part
    llm_analysis = match_resume_with_jd_llm(sample_resume, sample_jd)
    if llm_analysis:
        print("\n--- LLM Analysis Result ---")
        print(llm_analysis)
    else:
        print("\nLLM analysis failed (check API key, client setup, and potential errors above).")
